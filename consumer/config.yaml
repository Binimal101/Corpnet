# Consumer Configuration
#
# Embedding + LLM are real (needed for query-time embedding & answer
# generation).  Stores are stubs — the consumer does NOT touch
# archrag.db.  These will be swapped for network adapters later.

embedding:
  adapter: openai
  model: text-embedding-3-small
  dimension: 1536

llm:
  adapter: openai
  model: gpt-4o-mini
  temperature: 0.0

# Storage is stubbed — no local database.
# These keys exist so the config loader can recognise the adapter
# names and instantiate the right classes.
graph_store:
  adapter: stub

document_store:
  adapter: stub

vector_index:
  adapter: stub

clustering:
  adapter: stub

# Retrieval params still matter (they tune search behaviour once
# stubs are replaced with real network adapters).
retrieval:
  k_per_layer: 5
